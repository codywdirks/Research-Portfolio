# -*- coding: utf-8 -*-
"""Causal_Forest_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yj3xDRt4K_psFTCaGw9ei0FqyLQepE7e

# **Code for Sources of Labor Market Discrimination**

Note: There will be minimal data cleaning within this file, as the data was thoroughly cleaned prior to the beginning of the semester. This file is dedicated exclusively to the analysis.
"""

# Import Packages
!pip install econml

from econml.dml import CausalForestDML as CausalForest
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, r2_score
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Mount to Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Setting Working Directory
cd = "/content/drive/MyDrive/Data/"

"""##**(Kline et al., 2022)**

###**Data Preperation**
"""

# Importing Dataset
df = pd.read_csv(cd + "Kline_2022_data.csv")

# Making Sure That the Data Loaded Properly
df.head()

# Checking the Shape of the Data
df.shape

# Selecting State-Level Variables
state_vars = (
  'poverty_pop', 'immigrant_pop', 'economic_freedom_score', 'ginicoefficient', 'pct_white', 'pct_black',
  'pct_hispanic', 'population','rep_republican', 'rep_democrat', 'rep_other',
  'sen_republican', 'sen_democrat', 'sen_other', 'electoral_votes', 'percent_democrat', 'if_democrat', 'if_tied',
  'pct_homicide', 'pct_rape_revised', 'pct_robbery',
  'pct_aggravated_assault', 'pct_burglary', 'pct_larceny', 'pct_motor_vehicle_theft', 'pct_under_35', 'pct_over_65'
)

# Selecting Individual-Level Variables
individual_vars = ('female', 'over40', 'lgbtq_club', 'political_club', 'academic_club', 'gender_neutral_pronouns', 'associates')

# Combining the Two Sets of Variables
required_columns = list(state_vars) + list(individual_vars)

"""Variables in required columns represent the specific variables we are using for analysis. We separate the columns into individual-level and state-level variables to allow for easier adjustment between different studies."""

# Dropping N/A Observations
df = df.dropna(subset=required_columns)

# Check Number of Observations That Were Dropped
df.shape

# Splitting Data in Training and Testing
df_train, df_test = train_test_split(df, test_size=0.3, random_state=43)

"""### **Running the Training Model**"""

# Scaling Outcome Variable
scaler = StandardScaler()
df_train['cb'] = scaler.fit_transform(df_train[['cb']])

# Setting Variables for Causal Forest
X = df_train[list(required_columns)]
Y = df_train['cb'].values.astype(float)
W = df_train['black'].values.astype(float)

# Running the Training Forest
cf1 = CausalForest(model_t=RandomForestRegressor(n_estimators=100),model_y=RandomForestRegressor(n_estimators=100), n_estimators=1000, min_samples_leaf=50, random_state=42, max_features=5)
cf1.fit(Y, W, X=X)

# Changing Results Back to Variable Names
importance = cf1.feature_importances_
important_var = np.array(required_columns)[importance > 0]
print(important_var)

"""### **Running the Testing Model**"""

# Scaling Outcome Variable
df_test['cb'] = scaler.transform(df_test[['cb']])

# Setting Variables for Causal Forest
X_test = df_test[list(required_columns)]
Y_test = df_test['cb'].values.astype(float)
W_test = df_test['black'].values.astype(float)

# Running the Testing Forest
cf2 = CausalForest(model_t=RandomForestRegressor(n_estimators=100),model_y=RandomForestRegressor(n_estimators=100), n_estimators=1000, min_samples_leaf=100, random_state=42, max_features=10)
cf2.fit(Y, W, X=X)

# Obtaining CATE predictions for the test set
cate = cf2.effect(X_test)

# Calculating ATE
ate = cf2.ate(X_test)
print(ate)

"""### **Results - Best Linear Predictors**"""

predictors = df_test[required_columns].values
print("Shape of predictors:", predictors.shape)

predictors_df = pd.DataFrame(predictors, columns=required_columns, index=df_test.index)
predictors_aligned = predictors_df.loc[df_test.index]

predictors_aligned = sm.add_constant(predictors_aligned)

# Perform the regression analysis of CATE on aligned predictors
model = sm.OLS(cate, predictors_aligned)
results = model.fit()
print(results.summary())

"""##**(Neumark et al., 2019)**

###**Data Preperation**
"""

# Importing Dataset
df1 = pd.read_csv(cd + "Neumark_2019_data.csv", encoding='latin-1')

# Making Sure That the Data Loaded Properly
df1.head()

# Checking the Shape of the Data
df1.shape

# Selecting State-Level Variables
state_vars = (
  'poverty_pop', 'immigrant_pop', 'economic_freedom_score', 'ginicoefficient', 'pct_white', 'pct_black',
  'pct_hispanic', 'population','rep_republican', 'rep_democrat', 'rep_other',
  'sen_republican', 'sen_democrat', 'sen_other', 'electoral_votes', 'percent_democrat', 'if_democrat', 'if_tied',
  'pct_homicide', 'pct_rape_revised', 'pct_robbery',
  'pct_aggravated_assault', 'pct_burglary', 'pct_larceny', 'pct_motor_vehicle_theft', 'pct_under_35', 'pct_over_65'
)

# Selecting Individual-Level Variables
individual_vars = ('female', 'unemployed', 'highskill', 'spanish', 'grammar', 'college')

"""Here is a key for the variables found at the Individual-Level:

- female: A binary variable indicating whether an applicant is female (1 if female, 0 otherwise).

- unemployed: A binary variable indicating whether an applicant is currently unemployed (1 if unemployed, 0 otherwise).

- highskill: A binary variable indicating whether an applicant possesses high-level skills relevant to the job (1 if yes, 0 otherwise). **CHECK**

- spanish: A binary variable indicating whether an applicant is fluent in Spanish (1 if fluent, 0 otherwise).

- grammar: A binary variable indicating whether an applicant had grammatical errors in their application (1 if yes, 0 otherwise).

- college: A binary variable indicating whether an applicant has completed a college degree (1 if yes, 0 otherwise). **CHECK**
"""

required_columns = list(state_vars) + list(individual_vars)

len(required_columns)

df1 = df1.dropna(subset=required_columns)

# Removing N/A Variables
df1 = df1.dropna(subset=required_columns)

# Check Number of Observations That Were Dropped
df1.shape

# Splitting Data in Training and Testing
df1_train, df1_test = train_test_split(df1, test_size=0.3, random_state=43)

"""### **Running the Training Model**"""

# Scaling Outcome Variable
scaler = StandardScaler()
df1_train['cb'] = scaler.fit_transform(df1_train[['cb']])

# Setting Variables for Causal Forest
X = df1_train[list(required_columns)]
Y = df1_train['cb'].values.astype(float)
W = df1_train['old'].values.astype(float)

# Running the Training Forest
cf3 = CausalForest(model_t=RandomForestRegressor(n_estimators=100),model_y=RandomForestRegressor(n_estimators=100), n_estimators=1000, min_samples_leaf=50, random_state=42, max_features=5)
cf3.fit(Y, W, X=X)

# Changing Results Back to Variable Names
importance = cf3.feature_importances_
important_var = np.array(required_columns)[importance > 0]
print(important_var)

"""### **Running the Testing Model**"""

# Scaling Outcome Variable
df1_test['cb'] = scaler.transform(df1_test[['cb']])

# Setting Variables for Causal Forest
X_test = df1_test[list(required_columns)]
Y_test = df1_test['cb'].values.astype(float)
W_test = df1_test['old'].values.astype(float)

# Running the Testing Forest
cf4 = CausalForest(model_t=RandomForestRegressor(n_estimators=100),model_y=RandomForestRegressor(n_estimators=100), n_estimators=4000, min_samples_leaf=100, random_state=42, max_features=10)
cf4.fit(Y, W, X=X)

# Obtaining CATE predictions for the test set
cate = cf4.effect(X_test)

# Merging CATE predictions back into the test dataset
df1_test['cate'] = cate

# Calculating ATE
ate = cf4.ate(X_test)
print(ate)

"""### **Results**

#### **Best Linear Predictors**
"""

predictors = df1_test[required_columns].values
print("Shape of predictors:", predictors.shape)

predictors_df = pd.DataFrame(predictors, columns=required_columns, index=df1_test.index)
predictors_aligned = predictors_df.loc[df1_test.index]

predictors_aligned = sm.add_constant(predictors_aligned)

# Perform the regression analysis of CATE on aligned predictors
model = sm.OLS(cate, predictors_aligned)
results2 = model.fit()
print(results2.summary())